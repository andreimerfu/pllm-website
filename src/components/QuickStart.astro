---
import CodeBlock from './CodeBlock.tsx';
import '../styles/prism.css';

const deploymentOptions = [
  {
    title: "Kubernetes with Helm",
    description: "Production-ready deployment with auto-scaling",
    icon: "logos:kubernetes",
    badge: "Production",
    badgeColor: "from-blue-500 to-blue-600",
    code: `# Add the Helm repository
helm repo add pllm https://andreimerfu.github.io/pllm
helm repo update

# Install with your configuration
helm install pllm pllm/pllm \\
  --set pllm.secrets.jwtSecret="your-jwt-secret" \\
  --set pllm.secrets.masterKey="sk-master-key" \\
  --set pllm.secrets.openaiApiKey="sk-your-openai-key"

# Check status
kubectl get pods -l app.kubernetes.io/name=pllm`,
    language: "bash",
    features: ["High Availability", "Auto-scaling", "Monitoring", "Production Ready"]
  },
  {
    title: "Docker Compose",
    description: "Perfect for development and testing",
    icon: "logos:docker-icon",
    badge: "Development",
    badgeColor: "from-green-500 to-green-600",
    code: `# Clone and setup
git clone https://github.com/andreimerfu/pllm.git
cd pllm

# Configure environment
cp .env.example .env
echo "OPENAI_API_KEY=sk-your-key-here" >> .env

# Launch pLLM
docker compose up -d

# Test it works
curl http://localhost:8080/v1/chat/completions \\
  -H "Content-Type: application/json" \\
  -d '{"model": "gpt-3.5-turbo", "messages": [{"role": "user", "content": "Hello!"}]}'`,
    language: "bash",
    features: ["Quick Setup", "Local Development", "Easy Testing", "Full Stack"]
  },
  {
    title: "Binary Installation",
    description: "Lightweight deployment for simple setups",
    icon: "material-symbols:download",
    badge: "Minimal",
    badgeColor: "from-purple-500 to-purple-600",
    code: `# Download latest release
wget https://github.com/andreimerfu/pllm/releases/latest/download/pllm-linux-amd64

# Make executable
chmod +x pllm-linux-amd64

# Set environment variables
export OPENAI_API_KEY=sk-your-key-here
export JWT_SECRET=your-jwt-secret
export MASTER_KEY=sk-master-key

# Run pLLM
./pllm-linux-amd64 server`,
    language: "bash",
    features: ["No Dependencies", "Single Binary", "Fast Startup", "Cross Platform"]
  }
];

const integrationExamples = [
  {
    title: "Python",
    icon: "logos:python",
    code: `from openai import OpenAI

# Just change the base_url - that's it!
client = OpenAI(
    api_key="your-api-key",
    base_url="http://localhost:8080/v1"  # ← Point to pLLM
)

# Use exactly like OpenAI
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "Hello!"}]
)`,
    language: "python"
  },
  {
    title: "Node.js",
    icon: "logos:nodejs-icon",
    code: `import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: 'your-api-key',
  baseURL: 'http://localhost:8080/v1'  // ← Point to pLLM
});

const completion = await openai.chat.completions.create({
  model: "gpt-3.5-turbo",
  messages: [{role: "user", content: "Hello!"}]
});`,
    language: "javascript"
  },
  {
    title: "cURL",
    icon: "simple-icons:curl",
    code: `curl -X POST http://localhost:8080/v1/chat/completions \\
  -H "Content-Type: application/json" \\
  -H "X-API-Key: your-api-key" \\
  -d '{
    "model": "gpt-3.5-turbo",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'`,
    language: "bash"
  }
];
---

<section id="quick-start" class="section bg-white dark:bg-slate-900 transition-colors duration-200">
  <div class="container-page">
    <!-- Section header -->
    <div class="section-header">
      <h2 class="section-title">
        Get Started in Minutes
      </h2>
      <p class="section-description">
        Choose your deployment method and get pLLM running in your environment quickly.
      </p>
    </div>

    <!-- Deployment options -->
    <div class="mb-20">
      <h3 class="text-2xl font-bold text-slate-900 dark:text-white mb-8 text-center">Deployment Options</h3>
      
      <div class="grid grid-cols-1 xl:grid-cols-3 gap-6 lg:gap-8">
        {deploymentOptions.map((option) => (
          <div class="card card-hover p-4 sm:p-6 lg:p-8">
            <!-- Header -->
            <div class="flex items-start justify-between mb-6 gap-4">
              <div class="flex items-center flex-1 min-w-0">
                <div class="w-20 h-20 mr-6 flex-shrink-0 flex items-center justify-center bg-slate-50 dark:bg-slate-800 rounded-xl border border-slate-200 dark:border-slate-700">
                  <iconify-icon icon={option.icon} class="w-14 h-14 text-slate-700" style="font-size: 56px; width: 56px; height: 56px;"></iconify-icon>
                </div>
                <div class="min-w-0 flex-1">
                  <h4 class="text-lg sm:text-xl font-bold text-slate-900 dark:text-white">{option.title}</h4>
                  <p class="text-slate-600 dark:text-slate-400 text-sm leading-tight">{option.description}</p>
                </div>
              </div>
              <span class="badge-brand text-xs font-semibold flex-shrink-0 self-start">
                {option.badge}
              </span>
            </div>

            <!-- Features -->
            <div class="mb-6">
              <div class="grid grid-cols-1 sm:grid-cols-2 gap-2">
                {option.features.map((feature) => (
                  <div class="flex items-center text-sm text-slate-600 dark:text-slate-300">
                    <svg class="w-4 h-4 text-green-500 mr-2 flex-shrink-0" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                      <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"></path>
                    </svg>
                    <span class="truncate">{feature}</span>
                  </div>
                ))}
              </div>
            </div>

            <!-- Code -->
            <CodeBlock code={option.code} language={option.language} client:load />
          </div>
        ))}
      </div>
    </div>


    <!-- Integration examples -->
    <div>
      <div class="text-center mb-12">
        <h3 class="text-3xl font-bold text-slate-900 dark:text-white mb-4">Drop-in Integration</h3>
        <p class="text-lg text-slate-600 dark:text-slate-300 max-w-2xl mx-auto">
          pLLM is 100% OpenAI compatible. Just change your base URL and you're ready to go.
        </p>
      </div>

      <div class="grid grid-cols-1 lg:grid-cols-3 gap-6 lg:gap-8">
        {integrationExamples.map((example) => (
          <div class="card overflow-hidden">
            <!-- Header -->
            <div class="bg-slate-50 dark:bg-slate-800/80 border-b border-slate-200 dark:border-slate-700 p-4 sm:p-6">
              <div class="flex items-center">
                <div class="w-16 h-16 mr-6 flex-shrink-0 flex items-center justify-center bg-white dark:bg-slate-700 rounded-xl border border-slate-200 dark:border-slate-700">
                  <iconify-icon icon={example.icon} class="w-12 h-12" style="font-size: 48px; width: 48px; height: 48px;"></iconify-icon>
                </div>
                <h4 class="text-lg font-bold text-slate-900 dark:text-white">{example.title}</h4>
              </div>
            </div>

            <!-- Code -->
            <div class="p-4 sm:p-6">
              <CodeBlock code={example.code} language={example.language} client:load />
            </div>
          </div>
        ))}
      </div>

      <!-- Call to action -->
      <div class="text-center mt-12">
        <div class="inline-flex flex-col sm:flex-row gap-4">
          <a
            href="https://github.com/andreimerfu/pllm"
            target="_blank"
            rel="noopener noreferrer"
            class="btn-primary px-8 py-4 text-lg flex items-center justify-center"
          >
            <svg class="w-5 h-5 mr-2" fill="currentColor" viewBox="0 0 24 24">
              <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
            </svg>
            Start with GitHub
          </a>
          <a
            href="https://github.com/andreimerfu/pllm/tree/main/docs"
            target="_blank"
            rel="noopener noreferrer"
            class="btn-secondary px-8 py-4 text-lg flex items-center justify-center"
          >
            <svg class="w-5 h-5 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 6.253v13m0-13C10.832 5.477 9.246 5 7.5 5S4.168 5.477 3 6.253v13C4.168 18.477 5.754 18 7.5 18s3.332.477 4.5 1.253m0-13C13.168 5.477 14.754 5 16.5 5c1.746 0 3.332.477 4.5 1.253v13C19.832 18.477 18.246 18 16.5 18c-1.746 0-3.332.477-4.5 1.253"></path>
            </svg>
            Read Documentation
          </a>
        </div>
      </div>
    </div>
  </div>
</section>