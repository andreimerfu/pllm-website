---
import PerformanceChart from './PerformanceChart';
import { IconBox } from './FeatureIcons.tsx';
import type { FeatureIconName } from './FeatureIcons.tsx';

const benchmarkStats = [
  {
    metric: "Requests/sec",
    pllm: "12,000+",
    typical: "2,500",
    improvement: "4.8x",
    icon: "performance" as FeatureIconName
  },
  {
    metric: "P99 Latency",
    pllm: "0.8ms",
    typical: "15ms",
    improvement: "18.7x",
    icon: "latency" as FeatureIconName
  },
  {
    metric: "Memory Efficiency",
    pllm: "50-80MB",
    typical: "200-400MB",
    improvement: "4-8x",
    icon: "memory" as FeatureIconName
  },
  {
    metric: "Cold Start",
    pllm: "<100ms",
    typical: "2-5s",
    improvement: "20-50x",
    icon: "startup" as FeatureIconName
  }
];

const scalabilityFeatures = [
  {
    title: "True Parallelism",
    description: "No GIL limitations - utilize all CPU cores effectively",
    benefit: "Handle thousands of concurrent requests on a single instance"
  },
  {
    title: "Memory Efficient",
    description: "Native compilation with optimized memory management",
    benefit: "3-6x less memory usage compared to interpreted alternatives"
  },
  {
    title: "Instant Scaling",
    description: "Sub-100ms startup enables aggressive auto-scaling",
    benefit: "Scale from 0 to production load in milliseconds"
  },
  {
    title: "Network Optimized",
    description: "Efficient connection pooling and keep-alive management",
    benefit: "Minimal network overhead with connection reuse"
  }
];
---

<section id="benchmarks" class="section bg-slate-50 dark:bg-slate-900/50">
  <div class="container-page">
    <!-- Section header -->
    <div class="section-header">
      <h2 class="section-title">
        Performance Benchmarks
      </h2>
      <p class="section-description">
        Real-world performance data showing why Go-based pLLM outperforms interpreted gateway solutions.
      </p>
    </div>

    <!-- Benchmark stats grid -->
    <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-6 mb-16">
      {benchmarkStats.map((stat) => (
        <div class="card p-6">
          <div class="flex items-center justify-between mb-4">
            <IconBox name={stat.icon} size="sm" client:load />
            <span class="badge-success text-xs font-semibold">
              {stat.improvement} faster
            </span>
          </div>
          <h3 class="font-bold text-slate-900 dark:text-white mb-3">{stat.metric}</h3>
          <div class="space-y-2">
            <div class="flex justify-between items-center">
              <span class="text-sm text-slate-500 dark:text-slate-400">pLLM</span>
              <span class="font-semibold text-brand-600 dark:text-brand-400">{stat.pllm}</span>
            </div>
            <div class="flex justify-between items-center">
              <span class="text-sm text-slate-500 dark:text-slate-400">Typical</span>
              <span class="font-medium text-slate-400 dark:text-slate-500">{stat.typical}</span>
            </div>
          </div>
        </div>
      ))}
    </div>

    <!-- Interactive performance chart -->
    <div class="mb-16">
      <PerformanceChart client:load />
    </div>

    <!-- Load testing results -->
    <div class="mb-16">
      <div class="card p-8 lg:p-12">
        <div class="text-center mb-10">
          <h3 class="text-2xl sm:text-3xl font-bold text-slate-900 dark:text-white mb-4">Load Testing Results</h3>
          <p class="text-lg text-slate-600 dark:text-slate-400 max-w-2xl mx-auto">
            Stress tested with 10,000 concurrent users making chat completion requests.
          </p>
        </div>

        <div class="grid grid-cols-1 lg:grid-cols-2 gap-8">
          <!-- Test configuration -->
          <div class="space-y-6">
            <h4 class="text-xl font-bold text-slate-900 dark:text-white">Test Configuration</h4>
            <div class="bg-slate-50 dark:bg-slate-800/50 rounded-xl p-6 border border-slate-200 dark:border-slate-700">
              <div class="space-y-4">
                <div class="flex justify-between">
                  <span class="text-slate-600 dark:text-slate-400">Concurrent Users:</span>
                  <span class="font-semibold text-slate-900 dark:text-white">10,000</span>
                </div>
                <div class="flex justify-between">
                  <span class="text-slate-600 dark:text-slate-400">Request Type:</span>
                  <span class="font-semibold text-slate-900 dark:text-white">Chat Completions</span>
                </div>
                <div class="flex justify-between">
                  <span class="text-slate-600 dark:text-slate-400">Test Duration:</span>
                  <span class="font-semibold text-slate-900 dark:text-white">10 minutes</span>
                </div>
                <div class="flex justify-between">
                  <span class="text-slate-600 dark:text-slate-400">Infrastructure:</span>
                  <span class="font-semibold text-slate-900 dark:text-white">Single 4-core instance</span>
                </div>
                <div class="flex justify-between">
                  <span class="text-slate-600 dark:text-slate-400">Memory Limit:</span>
                  <span class="font-semibold text-slate-900 dark:text-white">1GB</span>
                </div>
              </div>
            </div>
          </div>

          <!-- Results -->
          <div class="space-y-6">
            <h4 class="text-xl font-bold text-slate-900 dark:text-white">Results</h4>
            <div class="space-y-4">
              <div class="bg-green-50 dark:bg-green-900/20 border border-green-200 dark:border-green-800 rounded-xl p-5">
                <div class="flex items-center mb-2">
                  <div class="w-2.5 h-2.5 bg-green-500 rounded-full mr-3"></div>
                  <span class="font-semibold text-green-800 dark:text-green-300">Success Rate</span>
                </div>
                <div class="text-3xl font-bold text-green-600 dark:text-green-400 mb-1">99.97%</div>
                <p class="text-sm text-green-700 dark:text-green-400/80">Zero failed requests under normal conditions</p>
              </div>

              <div class="bg-blue-50 dark:bg-blue-900/20 border border-blue-200 dark:border-blue-800 rounded-xl p-5">
                <div class="flex items-center mb-2">
                  <div class="w-2.5 h-2.5 bg-blue-500 rounded-full mr-3"></div>
                  <span class="font-semibold text-blue-800 dark:text-blue-300">Average Response Time</span>
                </div>
                <div class="text-3xl font-bold text-blue-600 dark:text-blue-400 mb-1">1.2ms</div>
                <p class="text-sm text-blue-700 dark:text-blue-400/80">Gateway overhead only, excluding LLM processing</p>
              </div>

              <div class="bg-purple-50 dark:bg-purple-900/20 border border-purple-200 dark:border-purple-800 rounded-xl p-5">
                <div class="flex items-center mb-2">
                  <div class="w-2.5 h-2.5 bg-purple-500 rounded-full mr-3"></div>
                  <span class="font-semibold text-purple-800 dark:text-purple-300">Memory Usage</span>
                </div>
                <div class="text-3xl font-bold text-purple-600 dark:text-purple-400 mb-1">78MB</div>
                <p class="text-sm text-purple-700 dark:text-purple-400/80">Peak memory during 10K concurrent connections</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Scalability features -->
    <div>
      <div class="text-center mb-12">
        <h3 class="text-2xl sm:text-3xl font-bold text-slate-900 dark:text-white mb-4">Enterprise Scalability</h3>
        <p class="text-lg text-slate-600 dark:text-slate-400 max-w-2xl mx-auto">
          Built-in scalability features that make pLLM ideal for high-volume production workloads.
        </p>
      </div>

      <div class="grid grid-cols-1 lg:grid-cols-2 gap-6">
        {scalabilityFeatures.map((feature) => (
          <div class="card p-6">
            <h4 class="text-lg font-bold text-slate-900 dark:text-white mb-2">{feature.title}</h4>
            <p class="text-slate-600 dark:text-slate-400 mb-4 leading-relaxed">{feature.description}</p>
            <div class="bg-brand-50 dark:bg-brand-900/20 rounded-xl p-4 border border-brand-100 dark:border-brand-800">
              <div class="flex items-start">
                <iconify-icon icon="solar:check-circle-bold-duotone" width="20" height="20" class="text-green-500 mr-3 mt-0.5 flex-shrink-0"></iconify-icon>
                <span class="text-sm font-medium text-slate-700 dark:text-slate-300">{feature.benefit}</span>
              </div>
            </div>
          </div>
        ))}
      </div>

      <!-- Enterprise scaling note -->
      <div class="mt-12 card bg-amber-50 dark:bg-amber-900/10 border-amber-200 dark:border-amber-800 p-8">
        <div class="flex items-start gap-6">
          <div class="flex-shrink-0">
            <div class="w-12 h-12 bg-amber-500 rounded-xl flex items-center justify-center">
              <iconify-icon icon="solar:danger-triangle-bold-duotone" width="24" height="24" class="text-white"></iconify-icon>
            </div>
          </div>
          <div>
            <h4 class="text-xl font-bold text-amber-900 dark:text-amber-200 mb-3">Enterprise Performance Scaling</h4>
            <p class="text-amber-800 dark:text-amber-300 mb-4 leading-relaxed">
              For <strong>massive performance and ultra-low latency</strong>, the bottleneck is often the LLM providers themselves, not the gateway.
              To achieve true enterprise scale:
            </p>
            <ul class="space-y-2 text-amber-800 dark:text-amber-300">
              <li class="flex items-start">
                <span class="w-2 h-2 bg-amber-500 rounded-full mr-3 mt-2 flex-shrink-0"></span>
                <span><strong>Multiple LLM Deployments:</strong> Deploy several instances of the same model (e.g., 5-10 GPT-4 Azure OpenAI deployments)</span>
              </li>
              <li class="flex items-start">
                <span class="w-2 h-2 bg-amber-500 rounded-full mr-3 mt-2 flex-shrink-0"></span>
                <span><strong>Multi-Provider Redundancy:</strong> Use multiple AWS Bedrock accounts, Azure regions, or provider accounts</span>
              </li>
              <li class="flex items-start">
                <span class="w-2 h-2 bg-amber-500 rounded-full mr-3 mt-2 flex-shrink-0"></span>
                <span><strong>Geographic Distribution:</strong> Deploy models across regions for latency optimization</span>
              </li>
            </ul>
            <p class="text-sm text-amber-700 dark:text-amber-400 mt-4 bg-amber-100 dark:bg-amber-900/30 rounded-lg p-3">
              <strong>Why This Matters:</strong> A single LLM deployment typically handles 60-100 RPM. For 10,000+ concurrent users,
              you need multiple deployments of the same model to prevent provider-side bottlenecks. pLLM's adaptive routing
              automatically distributes load across all deployments.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
